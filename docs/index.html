<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment between Game Video and User Operations with Vision Language Models</title>
</head>
<body>
    <h1>Alignment between Game Video and User Operations with Vision Language Models</h1>
    <h2>Overview</h2>
    <p>Game AI's evolution plays a critical role in shaping the technological landscape 
        and enriching user experiences. A key challenge in this domain is detecting and 
        interpreting user operations from video data, which can be leveraged to better 
        understand player strategies, improve AI adaptability, and enhance the 
        complexity of in-game AI responses. In this work, we implements the 
        <bold>Large-Models-Only (LMO)</bold> framework, which utilizes Vision Language 
        Models (VLMs), such as LLaVA, MiniCPM-V, and QWen2-VL, to detect these operations. 
        By proposing the LMO framework, we aim to provide a more robust and scalable 
        solution to capturing user operations across different games, ultimately 
        advancing the field of Game AI and paving the way for more dynamic, challenging, 
        and adaptive gaming environments. 
    </p>
    <p>This repository provides the code and resources necessary to evaluate these VLMs 
        for recognizing user actions within game video segments, enabling a more 
        sophisticated, non-intrusive approach to data collection. The LMO framework is 
        designed to overcome the limitations of traditional data collection methods, 
        which often involve direct tracking of user inputs and may influence player behavior.
    </p>
    
    <h2>Approach</h2>
    <p>We employ several state-of-the-art open-source Vision Language Models—LLaVA, 
        MiniCPM-V, and QWen2-VL—to analyze player actions in game footage. These models 
        bring together capabilities in computer vision and natural language processing, 
        capturing the visual and contextual nuances of gameplay without the need for 
        manual or intrusive data collection methods. Our LMO framework not only 
        increases scalability by reducing dependence on control customizations across 
        games but also introduces a high degree of generalizability for Game AI research.
    </p>


    <h2>Key Features</h2>
    <ul>
        <li>**Methodological Advantages of VLMs**: The paper delineates the specific 
            methodological advantages of using Vision-Language Models (VLMs) for 
            detecting user operations within game videos.</li>
        <li>**Enrichment of AI Training Datasets**: It highlights how VLMs can be leveraged 
            to enrich AI training datasets, potentially improving the quality and 
            diversity of the data used for developing Game AIs.</li>
        <li>**Enhancing Gaming Interactivity and Competitiveness**: The paper emphasizes 
            how the integration of VLMs could revolutionize the interactivity and 
            competitiveness of gaming environments, surpassing the limitations of 
            traditional data collection methods.</li>
        <li>**Implications for Future Game AI Research and Development**: The paper 
            discusses the broader implications of using VLMs for advancing Game AI 
            research and development, particularly in terms of increasing autonomy 
            and sophistication in Game AIs.</li>
    </ul>

    <h2>Prompt Design</h2>
    
    
</body>
</html>
